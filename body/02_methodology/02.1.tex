\subsection{Rationale for Method Selection}
When it comes to addressing classification problems, logistic regression stands out as a widely favored option, as well as decision trees and random forests. However, when it comes to identifying anomalies, the isolation forest algorithm is the preferred choice. This popularity stems from its simplicity and effectiveness in detecting anomalies within extensive datasets. Moreover, it operates as an unsupervised learning algorithm, eliminating the need for labeled data during the training process. This advantage is particularly crucial in applications like fraud detection, where obtaining labeled data is often challenging. Additionally, the isolation forest algorithm excels in detecting anomalies present in both numerical and categorical data, making it ideal for datasets that encompass diverse data types.

--- Phan nay can nhac cho vao ---

One way to tackle this problem could be to devise some hard-coded criteria for ‘normal’ transactions that are based on domain knowledge. For instance, we might know that transactions from account x on day y usually do not exceed a certain amount; so all transactions that do not fulfill this condition could be flagged as anomalies. The drawback of this type of approach is that we need to know in advance what an outlier is going to look like. In most cases it is not feasible, even for domain experts, to anticipate all the forms that an anomaly could assume, since the space of possible criteria is too vast to search manually. Luckily, this is a problem where machine learning can help us out. Instead of having to define certain criteria for outlier detection, we can build a model that learns these criteria (although less explicitly) by training on large amounts of data. In this article we are going to apply such a model, namely an Isolation Forest, and we will use an additional explanatory model that is going to help us make sense of the decisions our anomaly detection model makes.
--- Het ---